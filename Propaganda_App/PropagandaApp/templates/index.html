{% extends "base.html" %}
{%  block content %}
  <div class="jumbotron" style="text-align: center;">
    <h1 > Propaganda Detection and Annotation </h1>
  </div>
  <div style="text-align: center;">
    <p>This website allows you to both Detect Propaganda as well as Annotate Text Examples according to the Propaganda Techniques.</p>
    <p>Propaganda is a planned persuasive form of communication whose goal is to influence the opinions and mindset of a target audience or the public in general towards a specific agenda. Propaganda presents a definite tone while expressing its messages, which is a conscious act manifested by a certain individual, group or institution in an attempt to propagate their own narratives. With the advent of the internet and the rise in the number of social media platforms the spread of falsified information and distorted arguments in the form of propaganda has begun to spread on a massive scale. Propaganda can influence individuals by shaping their attitudes and beliefs about certain issues, events, or individuals. It can also influence their behavior, such as how they vote, what products they buy, or even how they view certain groups of people. Most work in propaganda detection has been done on high-resourced languages such as English, Arabic and Spanish. However, little effort has been made to detect propaganda on resource starved languages. Most low-resourced language communities' resort to mixing multiple languages especially on social media platforms in the form of either post, tweets or comments. This phenomenon of mixing of multiple languages is referred to as code-switching. Code-switching involves switching between two or more languages in a sentence or phrase to express their ideas and thoughts more accurately and convincingly. In general, code-switching brings together high-resourced and low-resourced languages within the same text. To contribute to a healthier online environment, we propose a novel task of detecting propaganda techniques in code-switched data involving English and Roman Urdu. We create a corpus of 1030 code-switched texts which we manually annotate on a fragment-level with 20 propaganda techniques and make it publicly available. For fragment-level annotation on our code-switched texts we develop a web-based annotation platform with an interface that allows easy labelling of spans of text. Furthermore, to do preliminary analysis on our newly created dataset we run experiments using several state-of-the-art pre-trained multilingual and cross-lingual language models namely BERT, mBERT and XLM RoBERTa with different fine-tuning strategies and discover that XLM RoBERTa fine-tuned on our task and dataset outperforms all the other models.</p>
  </div>
  <div>
    <img src="https://drive.google.com/uc?export=view&id=1eCsjNAtjXuXfqBLxeEnsBpOikUO06msr" style=" display: block; margin-left: auto; margin-right: auto; width: 50%; bottom: 10px;">
    
  </div>
{%  endblock %}