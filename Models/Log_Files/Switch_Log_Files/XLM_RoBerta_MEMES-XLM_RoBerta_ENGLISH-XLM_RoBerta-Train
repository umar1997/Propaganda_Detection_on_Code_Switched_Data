[WARNING] - META - domain_type: DEFAULT
[WARNING] - META - model_run: XLM_RoBerta_MEMES-XLM_RoBerta_ENGLISH-XLM_RoBerta
[WARNING] - META - model_type: default
[WARNING] - META - tokenizer_type: default
[WARNING] - META - training: True
[WARNING] - META - max_seq_length: 256
[WARNING] - META - random_seed: 42
[WARNING] - META - training_batch_size: 16
[WARNING] - META - validation_batch_size: 16
[WARNING] - META - learning_rate: 3e-05
[WARNING] - META - epsilon: 1e-08
[WARNING] - META - weight_decay: 0.1
[WARNING] - META - epochs: 5
[WARNING] - META - scheduler: LinearWarmup
[WARNING] - META - optimizer: AdamW
[WARNING] - META - max_grad_norm: 1.0
[WARNING] - META - full_finetuning: True
[WARNING] - META - debugging: False
[WARNING] - META - log_file: ./Switch_Files/XLM_RoBerta_MEMES-XLM_RoBerta_ENGLISH-XLM_RoBerta-Train
[WARNING] - META - datetime: 11-03-2023_17:07:46
[WARNING] - META - mode: SWITCHES

[WARNING] - META - Training Datatset: 688
[WARNING] - META - Validation Datatset: 89
[WARNING] - META - Testing Datatset: 155
[WARNING] - META - Vocab Size: 250002

[WARNING] - META - Domain Tyep: MEMES
[WARNING] - META - Model Type: xlm-roberta-base
[WARNING] - META - Tokenizer Type: xlm-roberta-base

[CRITICAL] - PORGRESS - 11-03-2023 17:07:56 - Model + Tokenizer Initialized
[CRITICAL] - PORGRESS - 11-03-2023 17:07:57 - Tokenizing sentences and encoding labels
[CRITICAL] - PORGRESS - 11-03-2023 17:07:57 - Data Loaders Created
[CRITICAL] - PORGRESS - 11-03-2023 17:07:57 - Training Started
[INFO] - RESULTS - training.py.<115> - Epoch #1
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 1: 0.42938214644443157
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 1: 0.46370913585027057
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.16713929017299803  |  Validation Exact Match Ratio: 0.0 |  Validation Accuracy Score: 0.8078651685393259
[INFO] - RESULTS - training.py.<208> - Duration: 12.803633689880371

[INFO] - RESULTS - training.py.<115> - Epoch #2
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 2: 0.2458075000796207
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 2: 0.34523441394170123
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.19930444087747456  |  Validation Exact Match Ratio: 0.011235955056179775 |  Validation Accuracy Score: 0.8612359550561798
[INFO] - RESULTS - training.py.<208> - Duration: 12.280936002731323

[INFO] - RESULTS - training.py.<115> - Epoch #3
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 3: 0.220702592370122
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 3: 0.3785901516675949
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.2593900481540931  |  Validation Exact Match Ratio: 0.0449438202247191 |  Validation Accuracy Score: 0.8471910112359551
[INFO] - RESULTS - training.py.<208> - Duration: 12.412588119506836

[INFO] - RESULTS - training.py.<115> - Epoch #4
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 4: 0.2097913213940554
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 4: 0.32227155069510144
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.252247191011236  |  Validation Exact Match Ratio: 0.033707865168539325 |  Validation Accuracy Score: 0.8786516853932584
[INFO] - RESULTS - training.py.<208> - Duration: 12.507436513900757

[INFO] - RESULTS - training.py.<115> - Epoch #5
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 5: 0.19644430435674134
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 5: 0.3048374801874161
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.17659176029962545  |  Validation Exact Match Ratio: 0.02247191011235955 |  Validation Accuracy Score: 0.8769662921348315
[INFO] - RESULTS - training.py.<208> - Duration: 12.577820539474487

[INFO] - RESULTS - training.py.<217> - Validation Hamming Score: 0.17659176029962545  |  Validation Exact Match Ratio: 0.02247191011235955 |  Validation Accuracy Score: 0.8769662921348315
[INFO] - RESULTS - training.py.<218> - Classification Report:
[INFO] - RESULTS - training.py.<219> - 
                                                     precision    recall  f1-score   support

                                    Loaded Language       0.42      0.78      0.54        40
      Obfuscation, Intentional vagueness, Confusion       0.00      0.00      0.00         1
                           Appeal to fear/prejudice       0.00      0.00      0.00         9
                                Appeal to authority       0.00      0.00      0.00         1
                                       Whataboutism       0.14      0.33      0.20         3
                                            Slogans       0.00      0.00      0.00         3
                          Exaggeration/Minimisation       0.00      0.00      0.00        30
               Black-and-white Fallacy/Dictatorship       0.00      0.00      0.00         3
                                             Smears       0.00      0.00      0.00        40
                                              Doubt       0.00      0.00      0.00         3
                                          Bandwagon       0.00      0.00      0.00         1
                              Name calling/Labeling       0.43      0.35      0.39        37
                               Reductio ad hitlerum       0.00      0.00      0.00         0
           Presenting Irrelevant Data (Red Herring)       0.00      0.00      0.00         5
                                         Repetition       0.00      0.00      0.00         1
Misrepresentation of Someone's Position (Straw Man)       0.00      0.00      0.00         2
                         Thought-terminating clich√©       0.00      0.00      0.00         4
                   Glittering generalities (Virtue)       0.00      0.00      0.00         4
                                        Flag-waving       0.00      0.00      0.00         3
                          Causal Oversimplification       0.00      0.00      0.00         8

                                          micro avg       0.41      0.23      0.29       198
                                          macro avg       0.05      0.07      0.06       198
                                       weighted avg       0.17      0.23      0.19       198
                                        samples avg       0.34      0.20      0.24       198

[CRITICAL] - PORGRESS - 11-03-2023 17:09:02 - Training Finished
[CRITICAL] - PORGRESS - 11-03-2023 17:09:02 - Model Saved



[WARNING] - META - Training Datatset: 786
[WARNING] - META - Validation Datatset: 89
[WARNING] - META - Testing Datatset: 155
[WARNING] - META - Vocab Size: 250002

[WARNING] - META - Domain Tyep: ENGLISH
[WARNING] - META - Model Type: xlm-roberta-base
[WARNING] - META - Tokenizer Type: xlm-roberta-base

[CRITICAL] - PORGRESS - 11-03-2023 17:09:13 - Model + Tokenizer Initialized
[CRITICAL] - PORGRESS - 11-03-2023 17:09:14 - Tokenizing sentences and encoding labels
[CRITICAL] - PORGRESS - 11-03-2023 17:09:14 - Data Loaders Created
[CRITICAL] - PORGRESS - 11-03-2023 17:09:14 - Training Started
[INFO] - RESULTS - training.py.<115> - Epoch #1
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 1: 0.2692796418070793
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 1: 0.3070726941029231
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.26431246655965757  |  Validation Exact Match Ratio: 0.0449438202247191 |  Validation Accuracy Score: 0.8685393258426967
[INFO] - RESULTS - training.py.<208> - Duration: 14.407301902770996

[INFO] - RESULTS - training.py.<115> - Epoch #2
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 2: 0.24053252071142198
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 2: 0.30154072244962055
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.3421348314606742  |  Validation Exact Match Ratio: 0.056179775280898875 |  Validation Accuracy Score: 0.8797752808988764
[INFO] - RESULTS - training.py.<208> - Duration: 14.15022325515747

[INFO] - RESULTS - training.py.<115> - Epoch #3
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 3: 0.21765333235263826
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 3: 0.27156853179136914
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.26704119850187263  |  Validation Exact Match Ratio: 0.056179775280898875 |  Validation Accuracy Score: 0.8926966292134831
[INFO] - RESULTS - training.py.<208> - Duration: 14.402363777160645

[INFO] - RESULTS - training.py.<115> - Epoch #4
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 4: 0.1956538164615631
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 4: 0.2603258366386096
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.23876404494382023  |  Validation Exact Match Ratio: 0.056179775280898875 |  Validation Accuracy Score: 0.8960674157303371
[INFO] - RESULTS - training.py.<208> - Duration: 14.903304815292358

[INFO] - RESULTS - training.py.<115> - Epoch #5
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 5: 0.17461503088474273
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 5: 0.2592171753446261
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.2539325842696629  |  Validation Exact Match Ratio: 0.11235955056179775 |  Validation Accuracy Score: 0.898314606741573
[INFO] - RESULTS - training.py.<208> - Duration: 14.884410619735718

[INFO] - RESULTS - training.py.<217> - Validation Hamming Score: 0.2539325842696629  |  Validation Exact Match Ratio: 0.11235955056179775 |  Validation Accuracy Score: 0.898314606741573
[INFO] - RESULTS - training.py.<218> - Classification Report:
[INFO] - RESULTS - training.py.<219> - 
                                                     precision    recall  f1-score   support

                                    Loaded Language       0.80      0.40      0.53        40
      Obfuscation, Intentional vagueness, Confusion       0.00      0.00      0.00         1
                           Appeal to fear/prejudice       0.00      0.00      0.00         9
                                Appeal to authority       0.00      0.00      0.00         1
                                       Whataboutism       0.00      0.00      0.00         3
                                            Slogans       0.00      0.00      0.00         3
                          Exaggeration/Minimisation       1.00      0.13      0.24        30
               Black-and-white Fallacy/Dictatorship       0.00      0.00      0.00         3
                                             Smears       0.50      0.28      0.35        40
                                              Doubt       0.00      0.00      0.00         3
                                          Bandwagon       0.00      0.00      0.00         1
                              Name calling/Labeling       0.51      0.49      0.50        37
                               Reductio ad hitlerum       0.00      0.00      0.00         0
           Presenting Irrelevant Data (Red Herring)       0.00      0.00      0.00         5
                                         Repetition       0.00      0.00      0.00         1
Misrepresentation of Someone's Position (Straw Man)       0.00      0.00      0.00         2
                         Thought-terminating clich√©       0.00      0.00      0.00         4
                   Glittering generalities (Virtue)       0.00      0.00      0.00         4
                                        Flag-waving       0.00      0.00      0.00         3
                          Causal Oversimplification       0.00      0.00      0.00         8

                                          micro avg       0.60      0.25      0.35       198
                                          macro avg       0.14      0.06      0.08       198
                                       weighted avg       0.51      0.25      0.31       198
                                        samples avg       0.32      0.23      0.26       198

[CRITICAL] - PORGRESS - 11-03-2023 17:10:29 - Training Finished
[CRITICAL] - PORGRESS - 11-03-2023 17:10:29 - Model Saved



[WARNING] - META - Training Datatset: 786
[WARNING] - META - Validation Datatset: 89
[WARNING] - META - Testing Datatset: 155
[WARNING] - META - Vocab Size: 250002

[WARNING] - META - Domain Tyep: CS
[WARNING] - META - Model Type: xlm-roberta-base
[WARNING] - META - Tokenizer Type: xlm-roberta-base

[CRITICAL] - PORGRESS - 11-03-2023 17:10:40 - Model + Tokenizer Initialized
[CRITICAL] - PORGRESS - 11-03-2023 17:10:40 - Tokenizing sentences and encoding labels
[CRITICAL] - PORGRESS - 11-03-2023 17:10:40 - Data Loaders Created
[CRITICAL] - PORGRESS - 11-03-2023 17:10:40 - Training Started
[INFO] - RESULTS - training.py.<115> - Epoch #1
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 1: 0.24214917093515395
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 1: 0.2513601283232371
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.24850187265917603  |  Validation Exact Match Ratio: 0.06741573033707865 |  Validation Accuracy Score: 0.897191011235955
[INFO] - RESULTS - training.py.<208> - Duration: 14.143418073654175

[INFO] - RESULTS - training.py.<115> - Epoch #2
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 2: 0.19986673474311828
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 2: 0.24910718450943628
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.2707865168539326  |  Validation Exact Match Ratio: 0.056179775280898875 |  Validation Accuracy Score: 0.9
[INFO] - RESULTS - training.py.<208> - Duration: 14.194116353988647

[INFO] - RESULTS - training.py.<115> - Epoch #3
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 3: 0.1657133962213993
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 3: 0.2652864456176758
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.3406367041198502  |  Validation Exact Match Ratio: 0.0898876404494382 |  Validation Accuracy Score: 0.900561797752809
[INFO] - RESULTS - training.py.<208> - Duration: 14.241531610488892

[INFO] - RESULTS - training.py.<115> - Epoch #4
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 4: 0.13254212588071823
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 4: 0.2852270230650902
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.27415730337078653  |  Validation Exact Match Ratio: 0.056179775280898875 |  Validation Accuracy Score: 0.8926966292134831
[INFO] - RESULTS - training.py.<208> - Duration: 14.296749114990234

[INFO] - RESULTS - training.py.<115> - Epoch #5
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 5: 0.11101596370339394
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 5: 0.30163265268007916
[INFO] - RESULTS - training.py.<196> - Validation Hamming Score: 0.29138576779026215  |  Validation Exact Match Ratio: 0.0449438202247191 |  Validation Accuracy Score: 0.8938202247191012
[INFO] - RESULTS - training.py.<208> - Duration: 14.333129644393921

[INFO] - RESULTS - training.py.<217> - Validation Hamming Score: 0.29138576779026215  |  Validation Exact Match Ratio: 0.0449438202247191 |  Validation Accuracy Score: 0.8938202247191012
[INFO] - RESULTS - training.py.<218> - Classification Report:
[INFO] - RESULTS - training.py.<219> - 
                                                     precision    recall  f1-score   support

                                    Loaded Language       0.59      0.55      0.57        40
      Obfuscation, Intentional vagueness, Confusion       0.00      0.00      0.00         1
                           Appeal to fear/prejudice       0.67      0.22      0.33         9
                                Appeal to authority       0.00      0.00      0.00         1
                                       Whataboutism       0.00      0.00      0.00         3
                                            Slogans       0.00      0.00      0.00         3
                          Exaggeration/Minimisation       0.52      0.43      0.47        30
               Black-and-white Fallacy/Dictatorship       0.00      0.00      0.00         3
                                             Smears       0.53      0.47      0.50        40
                                              Doubt       0.00      0.00      0.00         3
                                          Bandwagon       0.00      0.00      0.00         1
                              Name calling/Labeling       0.70      0.51      0.59        37
                               Reductio ad hitlerum       0.00      0.00      0.00         0
           Presenting Irrelevant Data (Red Herring)       0.00      0.00      0.00         5
                                         Repetition       0.00      0.00      0.00         1
Misrepresentation of Someone's Position (Straw Man)       0.00      0.00      0.00         2
                         Thought-terminating clich√©       0.00      0.00      0.00         4
                   Glittering generalities (Virtue)       0.00      0.00      0.00         4
                                        Flag-waving       0.00      0.00      0.00         3
                          Causal Oversimplification       0.20      0.12      0.15         8

                                          micro avg       0.53      0.38      0.45       198
                                          macro avg       0.16      0.12      0.13       198
                                       weighted avg       0.48      0.38      0.42       198
                                        samples avg       0.48      0.35      0.38       198

[CRITICAL] - PORGRESS - 11-03-2023 17:11:54 - Training Finished
[CRITICAL] - PORGRESS - 11-03-2023 17:11:54 - Model Saved



