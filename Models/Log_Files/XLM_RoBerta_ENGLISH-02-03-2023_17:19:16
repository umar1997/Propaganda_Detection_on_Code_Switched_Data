[WARNING] - META - domain_type: ENGLISH
[WARNING] - META - model_run: XLM_RoBerta_ENGLISH
[WARNING] - META - model_type: xlm-roberta-base
[WARNING] - META - tokenizer_type: xlm-roberta-base
[WARNING] - META - training: True
[WARNING] - META - max_seq_length: 256
[WARNING] - META - random_seed: 42
[WARNING] - META - training_batch_size: 16
[WARNING] - META - validation_batch_size: 16
[WARNING] - META - learning_rate: 3e-05
[WARNING] - META - epsilon: 1e-08
[WARNING] - META - weight_decay: 0.1
[WARNING] - META - epochs: 10
[WARNING] - META - scheduler: LinearWarmup
[WARNING] - META - optimizer: AdamW
[WARNING] - META - max_grad_norm: 1.0
[WARNING] - META - full_finetuning: True
[WARNING] - META - debugging: False
[WARNING] - META - log_file: ./Log_Files/XLM_RoBerta_ENGLISH-02-03-2023_17:19:16
[WARNING] - META - datetime: 02-03-2023_17:19:16

[WARNING] - META - Training Datatset: 786
[WARNING] - META - Validation Datatset: 89
[WARNING] - META - Testing Datatset: 155
[WARNING] - META - Vocab Size: 250002

[CRITICAL] - PORGRESS - 02-03-2023 17:19:26 - Model + Tokenizer Initialized
[CRITICAL] - PORGRESS - 02-03-2023 17:19:27 - Tokenizing sentences and encoding labels
[CRITICAL] - PORGRESS - 02-03-2023 17:19:27 - Data Loaders Created
[CRITICAL] - PORGRESS - 02-03-2023 17:19:27 - Training Started
[INFO] - RESULTS - training.py.<115> - Epoch #1
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 1: 0.39963646829128263
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 1: 0.30375860383113223
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8735955056179775  |  Validation Exact Match Ratio: 0.02247191011235955
[INFO] - RESULTS - training.py.<206> - Duration: 16.075438737869263

[INFO] - RESULTS - training.py.<115> - Epoch #2
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 2: 0.258273366689682
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 2: 0.2781823202967644
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8904494382022472  |  Validation Exact Match Ratio: 0.06741573033707865
[INFO] - RESULTS - training.py.<206> - Duration: 15.615867137908936

[INFO] - RESULTS - training.py.<115> - Epoch #3
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 3: 0.2184286481142044
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 3: 0.27539736529191333
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8926966292134831  |  Validation Exact Match Ratio: 0.07865168539325842
[INFO] - RESULTS - training.py.<206> - Duration: 15.191127300262451

[INFO] - RESULTS - training.py.<115> - Epoch #4
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 4: 0.1911811612546444
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 4: 0.2574179619550705
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.9028089887640449  |  Validation Exact Match Ratio: 0.11235955056179775
[INFO] - RESULTS - training.py.<206> - Duration: 15.20627212524414

[INFO] - RESULTS - training.py.<115> - Epoch #5
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 5: 0.1512606105208397
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 5: 0.28896351406971615
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.9028089887640449  |  Validation Exact Match Ratio: 0.11235955056179775
[INFO] - RESULTS - training.py.<206> - Duration: 15.213589906692505

[INFO] - RESULTS - training.py.<115> - Epoch #6
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 6: 0.1175091603398323
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 6: 0.2990038146575292
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.9044943820224719  |  Validation Exact Match Ratio: 0.11235955056179775
[INFO] - RESULTS - training.py.<206> - Duration: 15.275264978408813

[INFO] - RESULTS - training.py.<115> - Epoch #7
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 7: 0.08578287068754435
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 7: 0.33531272659699124
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.9078651685393259  |  Validation Exact Match Ratio: 0.11235955056179775
[INFO] - RESULTS - training.py.<206> - Duration: 15.776247501373291

[INFO] - RESULTS - training.py.<115> - Epoch #8
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 8: 0.059765746928751466
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 8: 0.345493550101916
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.9050561797752809  |  Validation Exact Match Ratio: 0.11235955056179775
[INFO] - RESULTS - training.py.<206> - Duration: 15.290002346038818

[INFO] - RESULTS - training.py.<115> - Epoch #9
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 9: 0.04581438187509775
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 9: 0.37346484263737995
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.9056179775280899  |  Validation Exact Match Ratio: 0.11235955056179775
[INFO] - RESULTS - training.py.<206> - Duration: 15.419464588165283

[INFO] - RESULTS - training.py.<115> - Epoch #10
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 10: 0.03454755794256926
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 10: 0.37253159284591675
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.9056179775280899  |  Validation Exact Match Ratio: 0.1348314606741573
[INFO] - RESULTS - training.py.<206> - Duration: 15.650335550308228

[INFO] - RESULTS - training.py.<215> - Validation Hamming Score: 0.9056179775280899  |  Validation Exact Match Ratio: 0.1348314606741573
[INFO] - RESULTS - training.py.<216> - Classification Report:
[INFO] - RESULTS - training.py.<217> - 
                                                     precision    recall  f1-score   support

                                    Loaded Language       0.59      0.68      0.63        40
      Obfuscation, Intentional vagueness, Confusion       0.00      0.00      0.00         1
                           Appeal to fear/prejudice       0.00      0.00      0.00         9
                                Appeal to authority       0.00      0.00      0.00         1
                                       Whataboutism       0.00      0.00      0.00         3
                                            Slogans       0.00      0.00      0.00         3
                          Exaggeration/Minimisation       0.77      0.33      0.47        30
               Black-and-white Fallacy/Dictatorship       0.00      0.00      0.00         3
                                             Smears       0.76      0.55      0.64        40
                                              Doubt       0.00      0.00      0.00         3
                                          Bandwagon       0.00      0.00      0.00         1
                              Name calling/Labeling       0.57      0.62      0.60        37
                               Reductio ad hitlerum       0.00      0.00      0.00         0
           Presenting Irrelevant Data (Red Herring)       0.00      0.00      0.00         5
                                         Repetition       0.00      0.00      0.00         1
Misrepresentation of Someone's Position (Straw Man)       0.00      0.00      0.00         2
                         Thought-terminating clich√©       0.00      0.00      0.00         4
                   Glittering generalities (Virtue)       0.00      0.00      0.00         4
                                        Flag-waving       0.00      0.00      0.00         3
                          Causal Oversimplification       0.00      0.00      0.00         8

                                          micro avg       0.61      0.41      0.49       198
                                          macro avg       0.13      0.11      0.12       198
                                       weighted avg       0.50      0.41      0.44       198
                                        samples avg       0.54      0.39      0.43       198

[CRITICAL] - PORGRESS - 02-03-2023 17:22:05 - Training Finished
[CRITICAL] - PORGRESS - 02-03-2023 17:22:05 - Model Saved
