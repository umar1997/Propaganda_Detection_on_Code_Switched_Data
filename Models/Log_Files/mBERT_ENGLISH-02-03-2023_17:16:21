[WARNING] - META - domain_type: ENGLISH
[WARNING] - META - model_run: mBERT_ENGLISH
[WARNING] - META - model_type: bert-base-multilingual-cased
[WARNING] - META - tokenizer_type: bert-base-multilingual-cased
[WARNING] - META - training: True
[WARNING] - META - max_seq_length: 256
[WARNING] - META - random_seed: 42
[WARNING] - META - training_batch_size: 16
[WARNING] - META - validation_batch_size: 16
[WARNING] - META - learning_rate: 3e-05
[WARNING] - META - epsilon: 1e-08
[WARNING] - META - weight_decay: 0.1
[WARNING] - META - epochs: 10
[WARNING] - META - scheduler: LinearWarmup
[WARNING] - META - optimizer: AdamW
[WARNING] - META - max_grad_norm: 1.0
[WARNING] - META - full_finetuning: True
[WARNING] - META - debugging: False
[WARNING] - META - log_file: ./Log_Files/mBERT_ENGLISH-02-03-2023_17:16:21
[WARNING] - META - datetime: 02-03-2023_17:16:21

[WARNING] - META - Training Datatset: 786
[WARNING] - META - Validation Datatset: 89
[WARNING] - META - Testing Datatset: 155
[WARNING] - META - Vocab Size: 119547

[CRITICAL] - PORGRESS - 02-03-2023 17:16:27 - Model + Tokenizer Initialized
[CRITICAL] - PORGRESS - 02-03-2023 17:16:28 - Tokenizing sentences and encoding labels
[CRITICAL] - PORGRESS - 02-03-2023 17:16:28 - Data Loaders Created
[CRITICAL] - PORGRESS - 02-03-2023 17:16:28 - Training Started
[INFO] - RESULTS - training.py.<115> - Epoch #1
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 1: 0.31538267731666564
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 1: 0.2952667127052943
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8865168539325843  |  Validation Exact Match Ratio: 0.0449438202247191
[INFO] - RESULTS - training.py.<206> - Duration: 14.60086965560913

[INFO] - RESULTS - training.py.<115> - Epoch #2
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 2: 0.2448444414138794
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 2: 0.2927229354778926
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8926966292134831  |  Validation Exact Match Ratio: 0.0898876404494382
[INFO] - RESULTS - training.py.<206> - Duration: 14.070947170257568

[INFO] - RESULTS - training.py.<115> - Epoch #3
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 3: 0.202132281512022
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 3: 0.36019959549109143
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8870786516853932  |  Validation Exact Match Ratio: 0.0
[INFO] - RESULTS - training.py.<206> - Duration: 14.465629816055298

[INFO] - RESULTS - training.py.<115> - Epoch #4
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 4: 0.1543680262565613
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 4: 0.3134140223264694
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.898314606741573  |  Validation Exact Match Ratio: 0.056179775280898875
[INFO] - RESULTS - training.py.<206> - Duration: 14.840941905975342

[INFO] - RESULTS - training.py.<115> - Epoch #5
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 5: 0.10595889642834663
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 5: 0.37334274252255756
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8870786516853932  |  Validation Exact Match Ratio: 0.056179775280898875
[INFO] - RESULTS - training.py.<206> - Duration: 14.54300332069397

[INFO] - RESULTS - training.py.<115> - Epoch #6
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 6: 0.06962600775063038
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 6: 0.39895637333393097
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8932584269662921  |  Validation Exact Match Ratio: 0.0449438202247191
[INFO] - RESULTS - training.py.<206> - Duration: 14.338676929473877

[INFO] - RESULTS - training.py.<115> - Epoch #7
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 7: 0.043688941225409506
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 7: 0.46317621568838757
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8921348314606742  |  Validation Exact Match Ratio: 0.033707865168539325
[INFO] - RESULTS - training.py.<206> - Duration: 14.325602531433105

[INFO] - RESULTS - training.py.<115> - Epoch #8
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 8: 0.03030165433883667
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 8: 0.4914504985014598
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8932584269662921  |  Validation Exact Match Ratio: 0.033707865168539325
[INFO] - RESULTS - training.py.<206> - Duration: 14.31172513961792

[INFO] - RESULTS - training.py.<115> - Epoch #9
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 9: 0.020818299725651742
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 9: 0.5136223385731379
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8915730337078651  |  Validation Exact Match Ratio: 0.033707865168539325
[INFO] - RESULTS - training.py.<206> - Duration: 14.321577787399292

[INFO] - RESULTS - training.py.<115> - Epoch #10
[INFO] - RESULTS - training.py.<148> - Average Train Loss For Epoch 10: 0.015244938833639025
[INFO] - RESULTS - training.py.<184> - Average Val Loss For Epoch 10: 0.5239856292804083
[INFO] - RESULTS - training.py.<194> - Validation Hamming Score: 0.8915730337078651  |  Validation Exact Match Ratio: 0.0449438202247191
[INFO] - RESULTS - training.py.<206> - Duration: 14.340387344360352

[INFO] - RESULTS - training.py.<215> - Validation Hamming Score: 0.8915730337078651  |  Validation Exact Match Ratio: 0.0449438202247191
[INFO] - RESULTS - training.py.<216> - Classification Report:
[INFO] - RESULTS - training.py.<217> - 
                                                     precision    recall  f1-score   support

                                    Loaded Language       0.46      0.47      0.47        40
      Obfuscation, Intentional vagueness, Confusion       0.00      0.00      0.00         1
                           Appeal to fear/prejudice       0.00      0.00      0.00         9
                                Appeal to authority       0.00      0.00      0.00         1
                                       Whataboutism       0.67      0.67      0.67         3
                                            Slogans       0.00      0.00      0.00         3
                          Exaggeration/Minimisation       0.71      0.17      0.27        30
               Black-and-white Fallacy/Dictatorship       0.00      0.00      0.00         3
                                             Smears       0.58      0.55      0.56        40
                                              Doubt       0.00      0.00      0.00         3
                                          Bandwagon       0.00      0.00      0.00         1
                              Name calling/Labeling       0.61      0.62      0.61        37
                               Reductio ad hitlerum       0.00      0.00      0.00         0
           Presenting Irrelevant Data (Red Herring)       0.00      0.00      0.00         5
                                         Repetition       0.00      0.00      0.00         1
Misrepresentation of Someone's Position (Straw Man)       0.00      0.00      0.00         2
                         Thought-terminating clich√©       0.00      0.00      0.00         4
                   Glittering generalities (Virtue)       0.00      0.00      0.00         4
                                        Flag-waving       0.00      0.00      0.00         3
                          Causal Oversimplification       0.00      0.00      0.00         8

                                          micro avg       0.52      0.36      0.42       198
                                          macro avg       0.15      0.12      0.13       198
                                       weighted avg       0.44      0.36      0.37       198
                                        samples avg       0.50      0.33      0.38       198

[CRITICAL] - PORGRESS - 02-03-2023 17:18:54 - Training Finished
[CRITICAL] - PORGRESS - 02-03-2023 17:18:54 - Model Saved
